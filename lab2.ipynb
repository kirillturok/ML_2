{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6mQlb3JMbuYlSnmmrkCZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirillturok/ML_2/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "gkuJFbQfmUNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get notMNIST data"
      ],
      "metadata": {
        "id": "NEdYe7s7nU33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "dataset_url = \"https://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\"\n",
        "dataset_dir = tf.keras.utils.get_file('notMNIST_large.tar', origin=dataset_url, extract=True)\n",
        "dataset_dir = pathlib.Path(dataset_dir).with_suffix('')"
      ],
      "metadata": {
        "id": "JvGGNBiz2DYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataframe"
      ],
      "metadata": {
        "id": "UsdvUhItmgGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "CLASSES = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
        "DATA_COLUMN = 'data'\n",
        "LABELS_COLUMN = 'labels'\n",
        "HASHED_DATA_COLUMN = 'hashed'\n",
        "\n",
        "def get_class_data(folder_path):\n",
        "    result_data = list()\n",
        "    files = os.listdir(folder_path)\n",
        "    for file in files:\n",
        "        image_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is not None:\n",
        "            result_data.append(img)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "def create_data_frame():\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    for class_item in CLASSES:\n",
        "        class_folder_path = os.path.join(dataset_dir, class_item)\n",
        "        class_data = get_class_data(class_folder_path)\n",
        "\n",
        "        data.extend(class_data)\n",
        "        labels.extend([CLASSES.index(class_item) for _ in range(len(class_data))])\n",
        "\n",
        "    data_frame = pd.DataFrame({DATA_COLUMN: data, LABELS_COLUMN: labels})\n",
        "\n",
        "    return data_frame\n",
        "\n",
        "data_frame = create_data_frame()\n"
      ],
      "metadata": {
        "id": "01tcr2VIepZ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "id": "xZOQg_nln4z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(data):\n",
        "    data_bytes = [item.tobytes() for item in data[DATA_COLUMN]]\n",
        "    data[HASHED_DATA_COLUMN] = data_bytes\n",
        "    data.sort_values(HASHED_DATA_COLUMN, inplace=True)\n",
        "    data.drop_duplicates(subset=HASHED_DATA_COLUMN, keep='first', inplace=True)\n",
        "    data.pop(HASHED_DATA_COLUMN)\n",
        "\n",
        "    return data\n",
        "\n",
        "df_no_duplicates = remove_duplicates(data_frame)\n",
        "\n",
        "min_class_count = df_no_duplicates[LABELS_COLUMN].value_counts().min()\n",
        "balanced_df = pd.concat([df_no_duplicates[df_no_duplicates[LABELS_COLUMN] == label].sample(min_class_count) for label in df_no_duplicates[LABELS_COLUMN].unique()])\n",
        "\n",
        "df = balanced_df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "N1p0DFds2mjJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide into subsamples"
      ],
      "metadata": {
        "id": "eXoZ1vtConov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.color import rgb2gray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def divide_into_subsamples(data_frame):\n",
        "    data = np.array(data_frame[DATA_COLUMN].values)\n",
        "    labels = np.array(data_frame[LABELS_COLUMN].values)\n",
        "\n",
        "    data_gray = np.array([rgb2gray(img) for img in data])\n",
        "    data_gray = data_gray.reshape(-1, 28*28)\n",
        "    data_gray = data_gray.astype('float32')\n",
        "\n",
        "    x_train, x_other, y_train, y_other = train_test_split(data_gray, labels, train_size=0.2, random_state = 10)\n",
        "    x_test, x_val, y_test, y_val = train_test_split(x_other, y_other, train_size = 0.5, random_state = 10)\n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, to_categorical(y_train, num_classes=10)))\n",
        "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, to_categorical(y_test, num_classes = 10)))\n",
        "    dataset_val = tf.data.Dataset.from_tensor_slices((x_val, to_categorical(y_val, num_classes = 10)))\n",
        "\n",
        "    return dataset_train, dataset_test, dataset_val\n",
        "\n",
        "dataset_train, dataset_test, dataset_val = divide_into_subsamples(df)\n",
        "\n",
        "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE)\n",
        "dataset_val = dataset_val.batch(BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "2L_0IPu3sOzG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "NBwr02sQj_r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models declaration"
      ],
      "metadata": {
        "id": "vABwlLxMrGu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_shape = (28, 28)\n",
        "num_classes = len(CLASSES)\n",
        "\n",
        "# Simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1. / 255),\n",
        "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Regularized model\n",
        "regularized_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1. / 255),\n",
        "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(\n",
        "        100,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(0.001)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "regularized_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "4NCmLR5v7tWi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters initialization"
      ],
      "metadata": {
        "id": "I-juFhzkuxZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "JyFJ6Em0u72B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple model processing"
      ],
      "metadata": {
        "id": "ilZaH5XRrMXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    dataset_train,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = dataset_val)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(dataset_val)\n",
        "print(f'\\nSimple Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')\n"
      ],
      "metadata": {
        "id": "ge7NvFOfq_BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularized model processing"
      ],
      "metadata": {
        "id": "i7K6G0bGsUHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regularized_model.fit(\n",
        "    dataset_train,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = dataset_val)\n",
        "\n",
        "test_loss, test_acc = regularized_model.evaluate(dataset_val)\n",
        "print(f'\\nRegularized Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')"
      ],
      "metadata": {
        "id": "TsH34V6qsZdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic model processing"
      ],
      "metadata": {
        "id": "q6u06FQuuF3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INITIAL_LEARNING_RATE = 0.01\n",
        "MIN_LEARNING_RATE = 1e-6\n",
        "DECAY_STEPS = 12000\n",
        "DECAY_RATE = 0.8\n",
        "\n",
        "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = INITIAL_LEARNING_RATE,\n",
        "    decay_steps = DECAY_STEPS,\n",
        "    decay_rate = DECAY_RATE,\n",
        "    staircase = True)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'val_loss',\n",
        "    factor = 0.1,\n",
        "    patience = 6,\n",
        "    verbose = 1,\n",
        "    min_lr = MIN_LEARNING_RATE)\n",
        "\n",
        "regularized_model.fit(\n",
        "    dataset_train,\n",
        "    validation_data = dataset_val,\n",
        "    epochs = EPOCHS,\n",
        "    callbacks = reduce_lr,\n",
        "    verbose = 1)\n",
        "\n",
        "test_loss, test_acc = regularized_model.evaluate(dataset_val)\n",
        "print(f'\\nDynamic Model\\n\\tTest Accuracy: {test_acc}\\n\\tTest Loss: {test_loss}')\n"
      ],
      "metadata": {
        "id": "Bo2pfCVx_daB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}